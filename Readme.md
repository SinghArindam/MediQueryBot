# ğŸ©º MediQueryBot

Glass-morphic medical chatbot powered by **Custom Fine-Tuned (LoRA) LLM**  
â€“ Multiple chats â€¢ Markdown answers â€¢ Collapsible reasoning â€¢ Latency footer

---

## âœ¨ Features
| Feature | Description |
| --- | --- |
| Custom Fine-Tuned (LoRA) LLM | Runs locally via `transformers` (CPU wheel, no API key). |
| Markdown rendering | Headlines, lists, code blocks, tables. |
| Multi-chat sidebar | Every chat stored as its own JSON file (`backend/chats/<id>.json`). |
| Reasoning reveal | Modelâ€™s `<think>` content collapses under â€œğŸ’­ Reasoningâ€. |
| Latency tracking | Response time is saved and shown. |
| AMOLED theme | pure-black UI with animated redâ†’blueâ†’greenâ†’gold halo. |

---

## ğŸƒâ€â™‚ï¸ Quick start (local)

```

git clone https://github.com/you/med-query-bot.git
cd med-query-bot

# build + run

docker build -t mediquery .
docker run -p 7860:7860 mediquery

# â†’ http://localhost:7860

```

### Python dev mode

```

python -m venv .venv \&\& source .venv/bin/activate
pip install -r backend/requirements.txt
uvicorn backend.main:app --reload

```
Open `frontend/index.html` directly or serve it with any static server.

---

## ğŸš€ Deploy to Hugging Face Spaces

1. Create a new **â€œDockerâ€** Space.  
2. Push the whole repo (`git push`).  
3. The Space builds automatically; first boot downloads the ~1.2 GB model.

No tokens required; Custom Fine-Tuned (LoRA) LLM.

---

## ğŸ“‚ Project structure
```

backend/         FastAPI + model loader
frontend/        HTML + Tailwind + JS (marked.js)
Dockerfile       Container for Spaces / local

```

---

## ğŸ”§ Config & tips
* Change `MODEL_ID` in `backend/main.py` to any Custom Fine-Tuned (LoRA) LLM model that fits RAM.  
* Restrict CORS (`allow_origins`) before going public.  
* Each assistant turn is stored with `"latency": seconds` â€“ useful for analytics.

---

Made with â¤ï¸ by **Arindam Singh**